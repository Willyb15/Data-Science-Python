{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For the past week you have been learning about new Python libraries: `Numpy` and `Pandas`. Up until now we haven't talked too much about what makes these libraries useful. Yeah, `Numpy` is good for vectorizing computations, and `Pandas` is good for loading and manipulating data; but are those tasks ends, in and of themselves, or means to an end? \n",
    "\n",
    "The answer, as with many of our answers, is, it depends. So far you've seen both libraries used independent of any particular task. Tonight that changes. Tonight we will use these libraries in efforts to accomplish the same end as [Pinky and the Brain](https://en.wikipedia.org/wiki/Pinky_and_the_Brain)! Ok, maybe be won't be trying to take over the world (...or will we?), but we will be working on just as lofty an endeavour: predicting the unknown.\n",
    "\n",
    "### Types of Learning\n",
    "\n",
    "In the realm of Data Science we talk about two distinct types of learning from data, [supervised](https://en.wikipedia.org/wiki/Supervised_learning) and [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning). There are other sub-classes of these two types of learning, but for the most part any data science pursuit can be classified into one of these two. **Unsupervised learning** consists of trying to find unapparent structure in your data that holds meaningful information. Today we are going to be focusing on **supervised learning**, though. This brand of learning attempts to build a model that predicts a target outcome from known inputs. This is accomplished through the use of inputs with correct targets to train the model. For example, if I'm trying to predict whether or not there will be a traffic jam on the highway. Data to solve this problem might have inputs such as the day of the week, the time of day, whether or not it's a holiday, if there is a major event going on that day, etc; and a target of traffic jam or not. Once the model is trained, we'll get to how to do this soon, I can then predict if there's going to be a traffic jam or not by plugging in the the desired inputs to the model. \n",
    "\n",
    "This was an example of what is known as a **binary classification** problem, the binary comes from deciding yes or no on traffic jam. Binary is just the simplest case of **classification problems**, though. Classification problems can tackle problems where you are trying to predict if inputs cause something to be in any of `n` classes. We will look at an example of this later with the iris data set.\n",
    "\n",
    "We don't always just want to classify something as a target for our predictions, though. Frequently we want predict how much of something will happen based on inputs. This type of problem is know as **regression**. An example of this is trying to predict the price of a stock at the end of a day based on how the market behaved that day. Here we aren't trying to classify the stock into a group but instead want a prediction for it's price, these values can take on any number on the real line, technically.\n",
    "\n",
    "### Types of Models\n",
    "\n",
    "Today we are going to get a taste for both classification and regession problems. The models that we're going to use in the examples are linear models, logistic and linear regression. These might sound simplistic, but they are far from it. Both of these types of models are the grand parents of statistical models and are a great place to start your journey towards learning about the great wide world of modeling. In addition, we will be using the `sklearn` library, this library has models that go well beyond the realm of linear. And because of polymorphism of classes they way that you will interact with those model classes will be very similar, in essence, to what you see in lecture today. Almost universally, there is both a `regression` and a `classification` version of the models found in `sklearn`. So, much of what you learn today will be applicable in a much larger scope than you may currently realize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problems\n",
    "\n",
    "As mentioned above, classification problems enter us into a realm where we want to predict what class something is in based on some inputs. To accomplish this we are going to want some data. Thank goodness we know how to load that stuff up. We are going to be working with the famous [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) data set today. It is located in the `sklearn.datasets` module. Let's take a look at it now. The data sets in `sklearn` have a description associated with them, let's look at that and the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "Iris Plants Database\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "print iris_data.data.shape\n",
    "print iris_data.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the description has a lot of information. But we could have gotton most of it and a better feel for the data if it were in a `dataframe`. So let's load it up into one! The names of the data colums are stored in the `feature_names` (features are a common name used to refer to inputs) attribute on the data object and the class are stored in the `target_names` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  target   label\n",
       "0           5.1          3.5           1.4          0.2       0  setosa\n",
       "1           4.9          3.0           1.4          0.2       0  setosa\n",
       "2           4.7          3.2           1.3          0.2       0  setosa\n",
       "3           4.6          3.1           1.5          0.2       0  setosa\n",
       "4           5.0          3.6           1.4          0.2       0  setosa"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "target_names = iris_data.target_names[iris_data.target]\n",
    "column_names = [name[:-5].replace(' ', '_') for name in iris_data.feature_names]\n",
    "iris_df = pd.DataFrame(iris_data.data, columns=column_names)\n",
    "iris_df['target'] = iris_data.target\n",
    "iris_df['label'] = target_names\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      "sepal_length    150 non-null float64\n",
      "sepal_width     150 non-null float64\n",
      "petal_length    150 non-null float64\n",
      "petal_width     150 non-null float64\n",
      "target          150 non-null int64\n",
      "label           150 non-null object\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 8.2+ KB\n"
     ]
    }
   ],
   "source": [
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width      target\n",
       "count    150.000000   150.000000    150.000000   150.000000  150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667    1.000000\n",
       "std        0.828066     0.433594      1.764420     0.763161    0.819232\n",
       "min        4.300000     2.000000      1.000000     0.100000    0.000000\n",
       "25%        5.100000     2.800000      1.600000     0.300000    0.000000\n",
       "50%        5.800000     3.000000      4.350000     1.300000    1.000000\n",
       "75%        6.400000     3.300000      5.100000     1.800000    2.000000\n",
       "max        7.900000     4.400000      6.900000     2.500000    2.000000"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at our data by plotting some of it. Here we're going to use a scatter plot of `sepal_length` vs `sepal_width` and color by the `label`. You'll easily be able to see that one of the classes is very different than the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1122f9690>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcQAAAG7CAYAAABdK+ULAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X2cVXW5///XtTeMAoPci3IzA4oWplF4juHdYapzLAzv\nUhK1TErzrrwlK9MvUpx+R7JDeoqULNMyj44n54g6qamTpsebUrAExbgbAkUBERiBgb2v3x9rz7DZ\nc7P3zKy999oz7+fjsR6svdaata79YTEX67M+61rm7oiIiPR0sWIHICIiEgVKiCIiIighioiIAEqI\nIiIigBKiiIgIoIQoIiICFCkhmtkqM1tsZq+Y2YttbHOLmb1pZovM7GOFjlFERHqWXkU6bhKocvf3\nWltpZlOAg939EDP7BHArMKmQAYqISM9SrC5Ty3LsU4C7ANz9BWCAmQ0vRGAiItIzFSshOvC4mb1k\nZhe0sn4ksCbt89rUMhERkbwoVpfpse7+lpkNI0iMS939Tx3diZmp7pyIiLTg7tbRnynKFaK7v5X6\n813gAeCojE3WAqPTPo9KLWttXyUzzZo1q+gxKNbiT6UUbynFWmrxKtb8TZ1V8IRoZn3NrDw13w84\nAfhbxmYPAuemtpkEbHb39QUNVEREepRidJkOBx5IdXf2Au5298fM7ELA3X2Buz9iZiea2d+BBmBG\nEeIUEZEepOAJ0d1XAi2eK3T32zI+f71gQRVIVVVVsUPImWLNn1KKt5RihdKKV7FGj3Wlv7XYzMxL\nOX4REQmfmeGdGFRTrFGmIiIlY8yYMaxevbrYYUiGyspKVq1aFdr+dIUoIpJF6oqj2GFIhrb+Xjp7\nhaji3iIiIighioiIAEqIIiIigBKiiIgIoIQoIiICKCGKiOTXsmXw05/CfffB9u3FjgaA1atXE4vF\nSCaTxQ4lUvQcoohIZ7nDM8/AihUwaRJ8+MN7r589G268MZjv1Qv69IFnn4Vx4wofaxp316MkrdAV\noohIZ2zZAkceCZ/7HHz96zBxIlx4YZAkAZYuDZLh9u3BtHUrbNgA55+/Zx/ucOedcPjhMGoUXHYZ\nbN7c4VBuvPFGRo0axX777cf48eN56qmncHf+4z/+g3HjxjFs2DCmT5/O5tS+J0+eDMDAgQPZb7/9\neOGFF3B35syZw5gxYzjggAM477zz2LJlCwA7d+7kS1/6EkOHDmXQoEF84hOf4N133wXgV7/6FYcd\ndhj77bcf48aNY8GCBV1o1CIr9ms6uviKDxcRybdWf9dccYX7Pvu4B2ktmPr1c6+tDdbPm9dyPbjH\nYu7JZLDN3LnuffvuWVdW5n744e6JRM6xvfHGGz569Gh/++233d199erVvmLFCv/xj3/sRx99tK9b\nt84bGxv9oosu8rPOOsvd3VetWuWxWMyTTXG4+y9+8Qs/5JBDfNWqVd7Q0OCf//zn/dxzz3V399tu\nu81PPvlk37FjhyeTSX/55Zd969at7u7+yCOP+MqVK93d/emnn/a+ffv6K6+80pHm7bS2ckBqecdz\nSmd+KCqTEqKIFEKrv2tGjWqZ7MB9xoxg/Z13upeXt1xfXh6s373bfb/9Wq7v39/98cdzju3vf/+7\nDx8+3P/whz/4rl27mpePHz/en3zyyebP69at8969e3sikfCVK1d6LBbzRFri/fSnP+0/+9nPmj+/\n8cYbXlZW5olEwn/5y1/6scce66+++mrWeE499VS/5ZZbco6/K8JOiOoyFRHpjP79Wy7r1QsGDQrm\nTzsNevcGS6sg1rcvfOMbwfwHHwRTpkQiuCeZo4MPPpgf//jH3HDDDey///6cffbZvPXWW6xevZrT\nTjuNwYMHM3jwYA477DB69+7N+vXrMWtZ1WzdunVUVlY2f66srGTXrl2sX7+eL33pS3zmM59h+vTp\njBo1im9/+9skEgkAamtrOfrooxkyZAiDBg2itraWDRs25Bx/lCghioh0xsyZQYJLV1YGF1wQzPfv\nHwy4OfJIiMWCbS+9FL7//WB9eTmMHNn6vo8+ukOhTJ8+nWeeeYb6+noAvvWtb1FRUUFtbS2bNm1i\n06ZNvPfeezQ0NHDggQe2mhBHjBixVwHz1atX07t3b4YPH06vXr24/vrree2113juuedYuHAhd911\nF42NjZxxxhlcc801vPvuu7z33ntMmTKlqQev5Cghioh0xowZ8J3vBImtrAxGjIDq6r1Hmn7kI/DS\nS7BjRzCoZu5ciMeDdWawYEGQKHulBvz36wfTpsERR+QcxrJly3jqqadobGykrKyMPn36EI/Hueii\ni7j22mubk+S7777Lgw8+CMCwYcOIxWIsX768eT9nnXUW8+bNY9WqVWzbto3vfve7TJ8+nVgsRl1d\nHX/7299IJpOUl5fTu3dv4vE4jY2NNDY2MnToUGKxGLW1tTz22GNda9ci0mMXIiKdYQbXXQff+lYw\n4nTw4L27R9P17t368hNOgEWLgsS4cSOcfjqceGKHwti5cyff/va3ef311+nduzfHHHMMCxYsYPjw\n4bg7J5xwAm+99Rb7778/Z555JieffDJ9+vThu9/9Lsceeyy7d+/m97//PV/5yld46623+Jd/+Rd2\n7tzJZz/7WW655RYA3n77bS666CLWrl1LeXk506dP54tf/CKxWIxbbrmFadOm0djYyEknncQpp5zS\nofijRK9/EhHJQs/sRZNe/yQiIpIHSogiIiIoIYqIiABKiCIiIoASooiICKCEKCIiAighioiIAEqI\nIiIigBKiiIjkoH///qxatapL+xg7dixPPvlkOAHlgUq3iYjk0bKNy3h8+eMM6zeMkw49iT69+xQ7\npE7ZunVrsUPIOyVEEZFOcneeqX+GFe+tYNKoSXx46If3Wj+7bjY3PnsjAL1ivejTqw/PfvVZxg0e\nV4xw25VIJIg3FR6PmELFpi5TEZFO2LJzC0cuOJLP/fZzfP2RrzPxtolc+NCFzbU1l767lBufvZHt\nu7ezffd2tjZuZcP2DZz/4PnN+3B37lx0J4fPP5xR/zmKy2ovY/OOzR2KY+7cuUybNm2vZZdffjlX\nXHEFW7Zs4atf/SojRoxg9OjRXH/99c3x3XnnnRx33HFcddVVDB06lNmzZ7N8+XKqqqoYOHAg+++/\nP2eddVbzPmOxGCtS72ncsWMHV199NWPGjGHQoEHNBcEBHnzwQQ4//HAGDx7Mpz71KV5//fVW425s\nbOSKK65g5MiRjBo1iiuvvJJdu3YB8Mc//pHRo0czd+5cDjzwQL7yla90qE06SwlRRKQTZtXNYsm7\nS9jWuI2GXQ1s372du1+9m0eXPwrAo8sfJenJvX4m6UmeqX+mOSnd9NxNXPLIJbz27mus3bqW2/5y\nG8ffcXyLn2vP9OnTqa2tpaGhIThGMkl1dTVnn3025513Hvvssw8rVqzglVde4fHHH+f2229v/tkX\nXniBcePG8c4773Dttddy/fXX85nPfIbNmzfzj3/8g280vcwY9nqH4tVXX80rr7zC888/z6ZNm5g7\ndy6xWIxly5Zx9tlnc8stt/Duu+8yZcoUTjrpJHbv3t0i7jlz5vDiiy/y6quvsnjxYl588UXmzJnT\nvP7tt99m8+bN1NfXs2DBgpzboyuUEEVEOuH+JfezM7Fzr2UNuxq477X7ABjcZzC94y1f+9S3d1/M\njEQywZxn5vDBrg+a1zUmGlm9eTVPrsx94ElFRQUTJ07kgQceAOCJJ56gX79+jBkzhkceeYR58+ax\n7777MnToUK644gruueee5p8dOXIkl1xyCbFYjH333ZfevXuzevVq1q5dS1lZGcccc0zztk1J3N25\n4447uOWWWzjggAMwMyZNmkTv3r257777mDp1Kp/61KeIx+PMnDmT7du389xzz7WI+7e//S2zZs1i\nyJAhDBkyhFmzZvHrX/+6eX08Hmf27Nn07t2bffbZJ+f26AolRBGRTuhf1r/Fsl7Wi0F9BgFw2odP\no3esN8aeK6u+vfvyjaOCq64Pdn2wVzJskvAEK95b0aFYzjrrrOZEd88993D22WezevVqdu3axYEH\nHsjgwYMZNGgQF110ERs2bGj+udGjR++1nx/+8Ickk0mOOuoojjjiCO64444Wx9qwYQM7d+7koIMO\narFu3bp1VFZWNn82M0aPHs3atWtb3baioqL5c2VlJevWrWv+PGzYMHq39R7JPFFCFBHphJnHzKRv\n7757LSvrVcYFEy8AoP8+/XlmxjMcOeJIYhajb+++XPrPl/L9T34fgPKyckb2H9nqvo8edXSHYpk2\nbRp1dXWsXbuWBx54gHPOOYfRo0ez7777snHjRjZt2sR7773H5s2befXVV5t/zjJeaLz//vuzYMEC\n1q5dy6233soll1zSfN+wydChQ9l3331Zvnx5izhGjBjB6tWr91q2Zs0aRo0alXXb1atXM2LEiDZj\nKwQlRBGRTpjxsRl857jvUF5WTlm8jBH9R1A9rXqvkaYf2f8jvHTBS+z47g62fmcrc/9tLvFYMFrS\nzFhw0gL69u5Lr1gw4L9f735MO2waRww/okOxDB06lMmTJzNjxgwOOuggDj30UA444ABOOOEErrzy\nSrZu3Yq7s2LFCp5++uk293P//fc3X80NHDiQWCxGLLZ3mjAzZsyYwVVXXcVbb71FMpnk+eefZ9eu\nXXzhC1/g4Ycf5qmnnmL37t3cdNNN7Lvvvhx9dMsEf9ZZZzFnzhw2bNjAhg0b+P73v8+XvvSlDn3v\n0Ll7yU5B+CIi+dXe75rG3Y2+oWGDJ5PJTu172YZlPvOxmT6jZoY/9MZDnd7Pr3/9a4/FYv6jH/2o\nedmWLVv84osv9lGjRvnAgQN94sSJfu+997q7+69+9Ss//vjj99rHNddc4yNHjvT+/fv7uHHj/Pbb\nb29eF4vFfPny5e7uvn37dr/yyit95MiRPnDgQJ88ebLv2LHD3d1ramr8sMMO84EDB3pVVZUvWbKk\neR9jx471J554wt3dd+zY4ZdffrkfeOCBPmLECL/iiit8586d7u5eV1fno0ePzvqd2/p7SS3vcE4x\nT90oLUVm5qUcv4iUBjNDv2uip62/l9TyDve5qstUREQEJUQRERFACVFERARQQhQREQGUEEVERAAl\nRBEREUAJUUREBFBCFBERAZQQRUS6rRNPPHGvN0jk8+e7eqwoUKUaEZEsulKpZtkyePxxGDYMTjoJ\n+vQJObgeLOxKNb1CiaoTzCwG/Bn4h7ufnLFuMvC/QFOZ9d+5+xxERCLEHZ55BlasgEmT4MMf3nv9\n7Nlw443BfK9eQTJ89lkYN67wsWZKJBLE4/FihxEpxewyvRxY0s76p919YmpSMhSRSNmyBY48Ej73\nOfj612HiRLjwwiBJAixdGiTD7duDaetW2LABzj9/zz7c4c474fDDYdQouOwy2Ly5Y3HMnTuXadOm\n7bXsiiuu4PLLL+eTn/wkv/zlLwG48847Oe6447jqqqsYOnQos2fPJplMcvXVVzNs2DAOPvhgfvrT\nnxKLxUgmkwAtfv7444/nm9/8JoMHD+bggw/m97//ffMx07cF+PnPf85hhx3Gfvvtx+GHH86iRYsA\nuPHGGxk3blzz8pqamo594TwqSkI0s1HAicDt7W1WoHBERDps1ixYsgS2bYOGhiDp3X03PPposP7R\nRyGVV5olk8EVZVPSvOkmuOQSeO01WLsWbrsNjj++5c+1Z/r06dTW1tLQ0JA6RpL77ruPc845p8W2\nL7zwAuPGjeOdd97hu9/9LgsWLODRRx/l1Vdf5eWXX6ampqbd9xC++OKLjB8/no0bN/LNb36Tr371\nq61uV11dzfe+9z1+85vfsGXLFh588EGGDBkCwLhx43j22WfZsmULs2bN4otf/CLr16/P/QvnUbGu\nEOcB3wTa65Q/2swWmdnDZnZYgeISEcnJ/ffDzp17L2togPvuC+YHD4bWXvjety+YQSIBc+bABx/s\nWdfYCKtXw5NP5h5HRUUFEydO5IEHHgDgiSeeoF+/fhx11FEtth05ciSXXHIJsViMffbZh+rqai6/\n/HIOPPBABgwYwLe//e12j1VZWclXvvIVzIwvf/nLvPXWW7zzzjsttvvFL37BNddcw8SJEwE46KCD\nGD16NACnn346w4cPB4IXGx9yyCG8+OKLuX/hPCr4PUQz+xyw3t0XmVkVrV8J/gWocPcPzGwKUAMc\n2tr+brjhhub5qqoqqqqqwg5ZRKSF/v1bLuvVCwYNCuZPOw2uuCJIfk1XhH37wje+Ecx/8MHeybBJ\nIhHck+yIs846i3vuuYcvfvGL3HPPPa1eHQLNSanJunXr9lqWuT7TAQcc0DzfJzU6aNu2bey///57\nbbdmzRoOPvjgVvdx1113MW/ePFatWgVAQ0MDGzZsaPe42dTV1VFXV9elfUBxBtUcC5xsZicCfYD+\nZnaXu5/btIG7b0ubrzWz+WY22N03Ze4sPSGKiBTKzJlBcktPamVlcMEFwXz//kH36Hnnwcsvw777\nwqWXwve/H6wvL4eRI4MrwkytvGC+XdOmTWPmzJmsXbuWBx54gBdeeKHV7TK7Qw888ED+8Y9/NH+u\nr6/v2IHbMHr0aJYvX95ieX19PV/72td46qmnODr1JT/+8Y93+V2TmRdDs2fP7tR+Ct5l6u7XunuF\nux8ETAeeTE+GAGY2PG3+KILHQ1okQxGRYpkxA77znSCxlZXBiBFQXb33SNOPfAReegl27AgG1cyd\nC00DO81gwYLgqrFX6tKkXz+YNg2OOKJjsQwdOpTJkyczY8YMDjroIA49tNUOtRa+8IUvcPPNN7Nu\n3To2b97M3LlzO3bgNpx//vncdNNNvPzyywAsX76cNWvW0NDQQCwWY+jQoSSTSe644w7+9re/hXLM\nMBTtsYtMZnYh4O6+ADjDzC4GdgHbgTOLGpyISAYzuO46+Na3ghGngwcHy1rT2r1EgBNOgEWLgsS4\ncSOcfjqceGLn4jn77LP58pe/zA9/+MO0GNsfm3jBBRfw5ptv8tGPfpQBAwZw2WWX8cc//pFYLJbT\nz6evT58/44wz2LRpE2effTbr1q1jzJgx/PrXv2bChAlcffXVTJo0iXg8zrnnnstxxx3Xma+bF3ow\nX0Qki648mF9Kfv/733PxxRezcuXKYoeSk7AfzFfpNhGRHmrHjh3U1taSSCRYu3Yts2fP5vOf/3yx\nwyoaXSGKiGTRXa8Qt2/fzuTJk3njjTfo06cPU6dO5cc//jHl5eXFDi0nYV8hKiGKuMPixcH8hAlt\n3wiSHqu7JsRS121qmYpEwtKlMGVKMKIBYMgQqK2F8eOLG5eIFJyuEKXncocxY2DNmj1PTptBRQWs\nXKkrRWmmK8Ro0qAakbAsXgybNu1JhhDMb9y4pwtVRHoMdZmKiGRRWVmZ9Zk8KbzKyspQ96cuU+m5\n3GHsWKivV5epSDeiLlORjjILBtBUVAT1t8rLg/naWiVDkR5IV4gieuxCpFvRc4giIiKoy1RERKRL\nlBBFRERQQhQREQGUEEVERAAlRBEREUAJUUREBFBCFBERAZQQRUREACVEERERQAlRREQEUEIUEREB\nlBBFREQAJUQREREAehU7AJFO02ubRCRESohSmpYuhSlTYOPG4POQIcGLfcePL25cIlKy9D5EKT3u\nMGYMrFkTzENwdVhRAStX6kpRpIfT+xCl51i8GDZt2pMMIZjfuHFPF6qISAcpIYqIiKCEKKVowoTg\nnmF616hZsGzChOLFJSIlTQlRSo9ZMICmogLKy4OpoiJYpvuHItJJGlQjpUuPXYhIKzo7qEYJUURE\nuhWNMhUREekCJUQRERGUEEVERACVbpN80GAXESlBSogSLtUYFZESpVGmEh7VGBWRCNAoUyk+1RgV\nkRKmhCgiIoISooRJNUZFpIQpIUp4VGNUREqYBtVI+PTYhYgUkWqZioiIoFGmIiIiXaKEKCIiQhET\nopnFzOxlM3uwjfW3mNmbZrbIzD5W6PhEmrnDokXBpC56kW6rmFeIlwNLWlthZlOAg939EOBC4NZC\nBibSbOlSGDsWjj8+mMaODZaJSLdTlIRoZqOAE4Hb29jkFOAuAHd/ARhgZsMLFJ5IwB0++1mor4dt\n24Kpvj6o1aorRZFup1hXiPOAbwJt/VYZCaxJ+7w2tUykcFSKTqRHKfjbLszsc8B6d19kZlVAlx5S\nu+GGG5rnq6qqqKqq6sruRESkxNTV1VFXV9fl/RT8OUQz+wHwRWA30AfoD/zO3c9N2+ZW4Cl3vzf1\n+XVgsruvz9iXnkOU/HEP7hnW1+vtHSIlpGSeQ3T3a929wt0PAqYDT6Ynw5QHgXMBzGwSsDkzGYrk\nnUrRifQokXlBsJldCLi7L3D3R8zsRDP7O9AAzChyeNJTjR8fXA2qFJ1It6fSbSIi0q2UTJepiIhI\nFCkhioiIEKF7iCItJJNQXR3MT5sGMf3/TUTyR/cQJZoeeghOPRUSieBzPA41NTB1anHjEpHI0/sQ\npftIJqGsbE8ybBKPQ2OjrhRFpF0aVCPdR3V1y2QIwbKmLlQRkZApIYqIiKAuU4kidZmKSBeoy1S6\nj1gsGEATj+9Z1jSoRslQRPJEV4gSXXrsQkQ6QaNMRUREUJepiIhIlyghioiIoNJt0hmJBNx0UzA/\nc+beg19Kkbte7yQlJaxTVqf+3nQPUTrmZz+DSy7Ze9n8+XDxxcWJp6uWLoUpU2DjxuDzkCHBC4DH\njy9uXCJtCOuU7c6nvgbVSP4lEtCrjU6F3btL70rRHcaMgTVrgnkI/otcURG8FLin/3dZIiesU7a7\nn/oaVCP519RN2tF1UbV4MWzatOc3AgTzGzfu6UcSiZCwTlmd+q1TQhQREUFdptIR3bHLdOxYqK/v\nnv1G0u2Edcp291NfXaaSf/F4MIAm0/z5pZcMIfhXX1sb/BYoLw+miopgWan/RpBuKaxTVqd+63SF\nKB2nxy5EikqPXbRPo0xFRERQl6mIiEiXKCGKiIig0m3SGWHceMhlH931BoeIRJISonRMGPWectlH\nd64rJSKRpEE1krsw6j3lso/uXldKRPJKg2ok/8Ko95TLPlRXSkSKQAlRREQEdZlKR4RR7ymXfXT3\nulIiklfqMpX8C6PeUy77UF0pESkCXSFKx+mxCxGJMJVuExERQV2mIiIiXaKEKCIigirVREOU7pVF\nKRaRAtKpLzknRDMbAUwFRgH7Zqx2d/9WmIH1GFEqURalWEQKSKe+QI6DaszsNOAeIA68AzRmbOLu\nflD44WWNq7QH1USpRFmUYhEpIJ363U++B9X8AHgMGO7uI919bMZU8GTYLUSpRFmUYhEpIJ360iTX\nLtPRwDfcfVM+gxERESmWXK8QnwM+lM9AeqQJE4KbFel9MmbBsgkTem4sIgWkU1+atJkQzaxv0wRc\nBXzNzL5sZiPS16VtIx0VpRJlUYpFpIB06kuTNgfVmFkSSF/ZdGq0+gPuHg83tOxKflBNkyiN945S\nLCIFpFO/+wi9dJuZnUcbya817n5nRw/eVd0mIYqISGhUy1RERIQ8P3ZhZivMrNXby2Z2uJmt6OiB\nRUREoiTXUaZjgH3aWNeXoHqNiIhIyWrzOUQz2w8YmLboADOryNhsX2A6sDbXA5rZPsDTQFnq+Pe7\n++yMbSYD/ws0XXn+zt3n5HoM6YJkEqqrg/lp0yDWyv+Zsm1TqNEJGgUhIiFqb1DNLGAW2QfWGHC1\nu8/L+aBmfd39AzOLA88Cl7n7i2nrJ6f2eXKW/egeYpgeeghOPRUSieBzPA41NTB1au7bFKoopIpP\nikgb8jHK9BDgUIKE9yAwE3gjY7NG4A13r+/ogVPH6EtwtXixu7+UtnwyMNPdT8ry80qIYUkmoaxs\nT6JrEo9DY2NwFZhtG7PCFIVU8UkRaUdnE2KbXabu/ibwZmrnnwRedvetnQ9xDzOLAX8BDgZ+mp4M\n0xxtZosIumO/6e5Lwji2tKG6umWig2BZdTWceWb2bT70ofaLQn7sY+HEmq34ZFjHEZEeJadapu7+\nxzAP6u5J4OOp+5Q1ZnZYRsL7C1CR6ladAtQQXK22cMMNNzTPV1VVUVVVFWaoIiIScXV1ddTV1XV5\nPx2pVNOuzlaqMbPrgQZ3/892tlkJHJlZXFxdpiEKq8t07Fior89/l2khjiMiJSkfzyFeljZdDawD\nlgE/BL4J3ETQpboutT7XQIea2YDUfB/g34DXM7YZnjZ/FEHi1ps28ikWCwbHxNP+X9M0YKZpFGm2\nbQpVFFLFJ0UkD3J9QfB/AhXAtPRLMjMzoBpY6+6X53RAsyOAOwmScQy4193/3cwuJHjR8AIzuxS4\nGNgFbAeudPcXWtmXrhDDpscuRKTE5bV0m5ltAM5x90dbWfcZ4LfuPqSjB+8qJUQREcmU19JtQBxo\n6wGvj3RgPyIiIpGU0yhT4G7gB2bWi+CZxHeA/YFTgO8Bv8hPeCIiIoWRa5dpGXAjcCF71zTdCSwA\nrnH3xrxE2H5c3aPLNKx7Ybnc/wtjH9nijdL3iRB3Z/H6oF0mDJ+AdaJdcmla3VqVnq6zXaa4e84T\nMBiYDJyZ+nNwR34+7CkIv8QtWeJeWeleXh5MlZXBso5auNA9HncPfh8G8wsXhr+PbPFG6ftEyJJ3\nlnjlvEov/0G5l/+g3CvnVfqSdzrWLrk0bVjNL1LKUrmhwzlF70MsprBKkOXyDGEY+8gWL0Tn+0SI\nuzPm5jGseX8Nnnq01zAqBlSw8vKVOV0p5nKqqKKdSCD0QTVmdmKqkkzTfLtTV4LvsbKVIMtVtpJq\nYe0jW7xR+j4Rsnj9YjZt39ScDAEcZ+P2jc1dqFn3kUPThtX8Ij1Ve4NqHgImAS+m5p2g0HdrnGAk\nqoiISElqr3RbJfCWuzem5tvl7qvDDi6bbtFlGkYJskJ2mbYXL0Tn+0SIuzP25rHUv1/fpS7TbE2r\ninYigdC7TN19tadGjqbm2526EnyPFVYJslzKroWxj2zxRun7RIiZUXtOLRUDKigvK6e8rJyKARXU\nnlOb80jTXJpWFe1Euqa9K8TXgT8B/wc86+6vt7phEZX8FWKTKD2moMcu8sb12IVIQeTjBcF3A0cD\nYwjuEb6dz3ShAAAgAElEQVRHKjmmppfcfUdnAw5Dt0mIIiISmrzVMjWzA4BjCZLjMcBEoIyg8PYr\nBMnxOXf/n44evKuUEEVEJFNei3tnHKgM+CeC5PivBK9v6vT7ELtCCVFERDJ1NiHmWsu06SCjCBLh\nMQRXjRMIXs/0UkcPLBGlm1QlL5l0qp9cBsC0Tx1KLFa8v59udhtYurn27iHGgY+zJwEeA4wCVhDc\nS2yaXnX3Vp6izj9dIYZs6VKYMiV4khtgyJBgiOL48R3bRormoWeXc+pJ+5BoGAhAvN9mahbuZOqx\nBxc+lofg1FP3PD3TNFB46tSChyI9TD4G1TSkZl8gSHzPA//n7hs6HWXIlBBDpNpgJS+ZdMqGriXx\n3gj2PFGVJD5oHY0bRhb0SrGbPUoqJSYf70PcCuwLDCd41dMwYGjnwpPIU22wklf95LLUlWH6P+sY\niYaBzV2oBYule1Xfkx6izXuI7n6AmR3EntGlXwduM7OtpK4WU9ML7r61EMGKiIjkS4dGmZpZOfAJ\nghqnx6TmBwKvufuEvETYfjzqMg2LaoOVPHWZigTy0WXagrtvA/4KvJaalhEU/D68oweWiFFtsJIX\nixk1C3cSH7QOyrZB2Tbig9ZRs3BnwUeadrPqe9JDtHuFaEFtqSPYe6TpWIIk+B5B1+lzBKXd6vId\nbCvx6QoxbHrsouTpsQvp6fIxyvQx4CigP0ECfJNUVRqCyjRLOh9uOJQQRUQkUz4ezN8XuJU9pdk2\ndjY4ERGRqOtw6bYo0RWiiIhkKkjpNmlFGPfTctlHoW7G6P5gp4TxaqdCynafsVCnQVi3rAv1ai3p\n5ty9ZKcg/CJassS9stK9vDyYKiuDZWHvY+FC93jcPfg3G8wvXBjOd+hoLNLCkneWeOW8Si//QbmX\n/6DcK+dV+pJ3ottuC//0d48PWuOUbXXKtnp80Bpf+Ke/N68v1GmQy3Fy2iaE9tep372kckOHc4q6\nTDsrjDJmueyjUA90qSxbp7g7Y24ew5r31+AE7WYYFQMqWHn5yshdKWZ7VtHMCnIahFUpMIz216nf\n/RTkOURJE0YZs1z2UagaWCrL1imL1y9m0/ZNzb+MARxn4/aNzV14UZKtvFuhToOwKgWG0f469aWJ\nEqKIiAjtP4d4SQf24+7+s3BCyl3Ru0y7WsYsl30UsstUZdk6zN0Ze/NY6t+v7zZdpoU4DcKqFBhG\n++vU734622Xa3oCVZAemRGduYHZ1QoNqwqWRBZ2iQTWdo0E1ki9oUE2RuB67ED120Vm5HCe3bfTY\nhewReum2UhCJhCgiIpFSkAfzzWwUcChBWbe9uPsjHT24iIhIVOSUEM2sP3AfcELTotSf6ZdncURE\nREpUrjei/j+gAjieIBmeBlQBvwBWErwwWDrLHRYtCqaudAFn208iATfeGEytPdsoPUZYp1w2iUSS\nG+9/lBvvf5REItnJfTg33rqKG29dRSKhWySSR7mMvAFWAGcTXAUmgX9OW/cj4L7OjOjp6kSxR5mG\nIazhbdn2M3/+nlGqTdP8+eF8BykphRpROf/hp5wBK52yLcE0YKXPf/ipju3jN6ud2C6HZDDFdvn8\n36wOP1jpVsjnKFMzawA+6+7PmNlW4Ax3fzS17tPA/7j7wPDTdda4PJf4I8tDqhmVbT/JJPRqo3d8\n9+69X2su3VpYp1w2iUSSXkPWwPujSX/ekQH17N5YQTyevXMqkXB69U6Ax9nrLk0swe7GOPG4hoFK\n6/Jdum0NMDw1/yYwNW3dJ4AdHT2wEF7NqGz7uemmtn+2vXXS7RSqTNlNDzwO2weTWSKO7UOCdbns\n4+erM5IhwXwyHqwTCVmuo0wfBz4N3A/MA+40syOBncC/EHSbioiIlKxcu0z7An3dfUPq82nAGUAf\ngmR5m7t37o55F3SLLtMwakZl24+6TCUlrFMum6DLtB7er6BLXaZlCUiqy1Q6Jq9dpu7+QVMyTH1+\nwN3PcffPu/vPipEMuwUzqK0NfhuVlwdTRUWwrCO/mbLtJx6H+fNb/tz8+UqGPUxYp1w28XiM+b9d\nBQPqoWxrMA2oZ/5vV+WUDIN9GPPvWgexBMETXkEynH/XOiVDyYsOVaoxsw8B/wwcCKwD/uzub+Qp\ntlziKe0rxCZh1YzKtp9EYs89w5kzlQx7sEKVKUskks33DGee9m85J8O99+HN9wxnXlCpZChZ5bV0\nm5ntB/wcOJ3gqnIbUE7wCMbvgPPdfUtHD95V3SYhiohIaPI9ynQ+QZWac4F+7r4f0A/4MvBvqfUi\nIiIlK9crxK3Ale5+eyvrLgD+09375yG+bHHpClFERPaS7+Le24C32li3DmjI9YBmtg/wNFCWOv79\n7j67le1uAaak9n2euy/K9RihCevdNIWS7RVRJfZ9PIRX+iSTSaqXBG0y7bBpxFp5bVY4x2n/dUrB\ncdpv2lz2USi53G7Opd2622umCiVbLFGKtVvJpZwNcB3wFNAnY3nf1PL/15HyOASPcEBQCu554KiM\n9VOAh1PznwCeb2M/OZfy6bCw3l5aKNleIlxi3yeMl74ufH2hx2fHnRtwbsDjs+O+8PW9X6wcynGy\nvHDXPXvT5rKPQsmlyl8u7dbdXkRcKNliiVKsUUWeS7f9EDiL4LVPjwPvAPsT3D/cDvw3e9584e7+\nrVyScer5xqeBi939pbTltwJPufu9qc9LgSp3X5/x855L/B2WS32rQtXAykUyCWVlLQt2x+PQ2BjE\nUkLfx90Zc/MY1ry/Bk+dVoZRMaCClZevzOkKLplMUjanjITv3SZxi9N4XSOxWCyk4zhlQ9eSeG8E\n6c/bxQeto3HDSGIxy9q07tn3USiJRPZHVnNpt2ztYmYFOd1K7Z9ytlggOrFGWb4H1ZwB7AK2ErzZ\n4uTUn1uB3an109KmbMHGzOwV4G3g8fRkmDKSoFxck7WpZYWRS32rQtXAykV1detvr0gkgnUl9n0W\nr1/Mpu2bmn/ZAjjOxu0bm7vosqleUt0iGQIkPNHchRrKcZ5cRqJhIJklyhINA5u7CrM1bS77KJRc\nqvzl0m7ZvlOhTrcSO/WzxhKlWLujnO4huvvYMA/qwYP8H089zlFjZoe5+5LO7OuGG25onq+qqqKq\nqiqUGEVEpDTU1dVRV1fX5f106MH8fDCz64EGd//PtGWZXaavA5ML2mWarb5VoWpg5SKXLtMS+j7u\nztibx1L/fn3eu0y7fpzcukzba9pS7DLN1m65dJkW4nQrtX/K2WKB6MQaZfnuMsXMPmpm95rZcjPb\naWYTU8v/3cymdGA/Q81sQGq+D8F9yNczNnuQ4JlHzGwSsDkzGeZVLvWtClUDKxexGNTU7D0UMB4P\nlsViJfd9zIzac2qpGFBBeVk55WXlVAyooPac2pxHgMZiMWrOrCFue9okbnFqzqxpHmkaznGMmoU7\niQ9aB2XboGwb8UHrqFm4szmRZWvaXPZRKLlU+cul3bJ9p0KdbiV26meNJUqxdke5DqqZQpCkngOe\nBGYB/+TuL5vZ/wMmufuJOR3Q7AjgToJkHAPudfd/N7MLCQbkLEht9xPgswSPXcxw95db2Vd+rhCb\nlNpYbT120YIeu+gcPXahxy5KWb5Lty0CXnL3C8ysF9DInoR4MnCru4/ocNRdpAfzRUQkU767TD8M\n3Juaz8xAW4DBHT2wiIhIlOSaEN8BDmpj3UeA+nDCERERKY5cE+J/A98zs+PSlrmZHQp8C7g79MhE\nREQKKNd7iPsA/0NQUu1tgvch/gM4AHgMOM3dd+UxzrbiKv49RN3djrRcBn6EMagmjFjCijW3ASRd\n/86Farco0T/30pDXQTVpB/k08GlgKLAJeMLdH+/oQcNS9IS4dClMmRKUiQAYMiQY/zx+fPFikmZL\n313KlLunsHF78PczpM8Qas+pZfyw8R3aphCxhBVrLqdkGN+5UO0WJfrnXjoKkhCjpqgJMUoFEKWF\nXOpthlHLNIxYgFBiza1uZ9e/c6HaLUr0z7205P3B/LQD9TWzb5jZT83sejOr7Og+ugUVFYy0XOpt\nhlHLNIxYwoo1p7qdIXznQrVblOife8/QZi1TM/sRcJK7H5q2rD/wEnAI8B4wALjazI5y98JWIRYR\nEQlRe1eInwR+k7FsJnAocIG7DwVGAKuA6/MSXZRNmBDcREjvKzELlk2YULy4BAgGeQzpMwRjz9+P\nYQzpM4QJwyfkvE0hYgkr1lxOyTC+c6HaLUr0z71naC8hjgH+krHsdGCJu/8SwN3fBX4EHJuX6KJM\nRQUjLZd6m2HUMg0jlrBiza1uZ9e/c6HaLUr0z71naHNQjZltIXic4onU58HAu8BP3f2ytO2OBx51\n974FiDczRj12Ie3SYxd67CJM+udeGkIfZWpmfwZq3f361OdzgTuAk9394bTtTgdudvdRnYq8CyKR\nEEVEJFI6mxDbe0HwT4Cfp17VtB64DFhJ8CB+uhOAv3X0wCIiIlHSZkJ091+Z2YHApcBA4GXg0vSK\nNGY2DDgFmJ3vQEVERPJJD+ZLt5bLPZ9s70ws1H2j3N7dGM57/bK/OjMa91VLTZTe79iTqVKNSIZc\nSm099MZDnHrvqSQ8AUDc4tScWcPUD03NeR9hyBZHrrHk9J0fglNPDV4CDMHLf2tqYGrqUFEpZ1dq\nCnWuqIRcdkqIImlyKbWVTCYpm1PWnISaxC1O43WNmMUKUq4rWxyxWCzHsmy5fGcoK9uTDJuPFYfG\nRjCLRjm7UrtSLFRpN5WQy03BSreJlIJcSm1VL6lukYQAEp6gekl1wcp1ZYsj1++T03eubpkMIVhW\nXR2dcnalplDnikrI5ZcSooiICOoylW7KHcaOhfr6rnWZZttHGHLtMs0WS27fOXuX6dibx1L/fn3e\nu0wLcZxCyaXtS+k4pU5dpiJpcim1FYvFqDmzhrjFm3+uaTBLLBYrWLmubHHk+n1y+87BAJr4nkM1\nD6qJxaJTzq7UFOpcUQm5/NIVonRreuxCj10Ukh67iAaNMhUREUFdpiIiIl2ihCgiIkL7xb1FOiVK\n94XCiGX37iRXz3segB9dOYlevTr3/8jC3ZcL5z6jSE+je4gSqiiV4wojlv9364t8/5KPg6f+72i7\nuX7+K3zvoqMKHktOxwmpvJtIKdOgGim6KJXjCiOW3buT9C5LgseBpu0dbDe7GuM5XykWql3CKu8m\nUuo0qEaKLkrluMKI5ep5z2ckQ4J579XchVqoWHI6Tkjl3UR6KiVEERERlBAlRBOGT2BInyFY2hWV\nYQzpM4QJwyeUXCw/unIS2G4gvVs+6DL90ZWTChpLTseZENwPTO/2NAuWTZiQ+zYiPZUSooQmSuW4\nwoilV68Y189/JS0pevOgmo6MNC1cObRwyruJ9FQaVCOh02MX+Yslt+PosQvp2TTKVEREBI0yFRER\n6RIlRBEREVS6TUpYWPfksu0nSvf+REDnSr4oIUpJCqsUWrb9RKnkmgjoXMknDaqRkhNWKbRs+wEi\nU3JNBHSu5EqDaqTHCKsUWrb9RKnkmgjoXMk3JUQRERGUEKUEhVUKLdt+olRyTQR0ruSbEqKUnLBK\noWXbT5RKromAzpV806AaKVl67EJ6Kp0r7VPpNhERETTKVEREpEuUEEVERChCQjSzUWb2pJm9ZmZ/\nNbPLWtlmspltNrOXU9N1hY6zO3KHRYuCqSs9ze7OorcXsejtRRSzyzqXOHLbJpx2iYpk0rn3D29w\n7x/eIJks7hfqbm0r3VvB7yGa2QHAAe6+yMzKgb8Ap7j762nbTAaudveTs+xL9xBzFFa5p0KVMgsj\njpy26WZlsB56djmnnrQPiYaBAMT7baZm4U6mHntwwWPpbm0rpaNkB9WYWQ3wX+7+RNqyycBMdz8p\ny88qIeYgrHJPYZVM66pc4shtm+5VBiuZdMqGriXx3gj2dP4kiQ9aR+OGkcRihftC3a1tpbSU5KAa\nMxsDfAx4oZXVR5vZIjN72MwOK2hg3UxY5Z4KVcosjDhy2qablcGqfnJZ6sow/Z91jETDQKqfXFbQ\nWLpb20rPULS3XaS6S+8HLnf3bRmr/wJUuPsHZjYFqAEObW0/N9xwQ/N8VVUVVVVVeYlXRESiqa6u\njrq6ui7vpyhdpmbWC3gIqHX3m3PYfiVwpLtvyliuLtMcuMPYsVBf3/Uu07E3j6X+/fqid5lmiyO3\nbcJpl6iIWpdpd2pbKS2l1mX6S2BJW8nQzIanzR9FkLg3tbatZBdWuadClTILI47ctuleZbBiMaNm\n4U7ig9ZB2TYo20Z80DpqFu4saDKE7te20jMUY5TpscDTwF8BT03XApWAu/sCM7sUuBjYBWwHrnT3\nFvcZdYXYMWGVeypUKbMw4shtm+5VBiuZ9OZ7htM+dWjBk2G67ta2UhpKdpRpVyghiohIplLrMhUR\nEYkUJUQRERGK+NiFFF5U7v2FJZlMUr2kGoBph00jFtP/70Sk83QPsYeISsm1sDz0xkOceu+pJDwB\nQNzi1JxZw9QPTS1yZCJSbBpUI22KSsm1sCSTScrmlDUnwyZxi9N4XaOuFEV6OA2qkTZFpeRaWKqX\nVLdIhgAJTzR3oYqIdJQSooiICEqIPcKE4RMY0mcIxp4eBMMY0mcIE4ZPKGJknTPtsGnELd5iedzi\nTDtsWhEiEpHuQAmxB4hKybWwxGIxas6s2SspNg2q0f1DEeksDarpQfTYhYj0BBplKiIigkaZioiI\ndIkSooiICEqIIiIigGqZFkYJvRSulAbelFKshaR2EekcJcR8W7oUpkyBjUENUYYMCV4bPj56NURL\nqd5pKcVaSGoXkc7TKNN8cocxY2DNmmAegqvDigpYuTJSV4qlVO+0lGItJLWLSECjTKNo8WLYtGlP\nMoRgfuPGPV2oEVFK9U5LKdZCUruIdI0SooiICEqI+TVhQnDPML2ryixYNiFaNURLqd5pKcVaSGoX\nka5RQswns2AATUUFlJcHU0VFsCxi93NKqd5pKcVaSGoXka7RoJpC0GMXeVFKsRaS2kV6OtUyFRER\nQaNMRUREukQJUUREBFWqESmYZNKpfnIZANM+dSixWMfv7en+oEj+6B6iSAE89OxyTj1pHxINAwGI\n99tMzcKdTD324Jz3obJsIrnRoBqRiEomnbKha0m8N4I9dymSxAeto3HDyJyuFFWWTSR3GlQjElHV\nTy5LXRmm/3OLkWgY2NyFmo3KsonknxKiiIgI6jIVybuwukzH3jyW+vfr1WUqkoW6TEUiKhYzahbu\nJD5oHZRtg7JtxAeto2bhzpxHmqosm0j+6QpRpED02IVIYWiUqYiICOoyFRER6RIlRBEREZQQRURE\nACVEERERQAlRREQEUEIUEREBlBBFREQAJUQRERFACVFERARQQhQREQGUEEVERAAlRBEREUAJUURE\nBChCQjSzUWb2pJm9ZmZ/NbPL2tjuFjN708wWmdnHCh1nT+XuLHp7EYveXoTeJCIiPUmvIhxzN3CV\nuy8ys3LgL2b2mLu/3rSBmU0BDnb3Q8zsE8CtwKQixNqjLH13KVPunsLG7RsBGNJnCLXn1DJ+2Pgi\nRyYikn8Fv0J097fdfVFqfhuwFBiZsdkpwF2pbV4ABpjZ8IIG2sO4O5+9+7PUv1/PtsZtbGvcRv37\n9Uy5e4quFEWkRyjqPUQzGwN8DHghY9VIYE3a57W0TJoSosXrF7Np+yacPcnPcTZu39j8hnYRke6s\nGF2mAKS6S+8HLk9dKXbKDTfc0DxfVVVFVVVVl2MTEZHSUVdXR11dXZf3Y8XoDjOzXsBDQK2739zK\n+luBp9z93tTn14HJ7r4+YztXd1443J2xN4+l/v365qtEw6gYUMHKy1diZkWOUEQkN2aGu3f4l1ax\nukx/CSxpLRmmPAicC2Bmk4DNmclQwmVm1J5TS8WACsrLyikvK6diQAW159QqGYpIj1DwK0QzOxZ4\nGvgr4KnpWqAScHdfkNruJ8BngQZghru/3Mq+dIUYMndvvmc4YfgEJUMRKTmdvUIsSpdpWJQQRUQk\nU6l1mYqIiESKEqKIiAhKiCIiIoASooiICKCEKCIiAighioiIAEqIIiIigBKiiIgIoIQoIiICKCGK\niIgASogiIiKAEqKIiAighCgiIgIoIYqIiABKiCIiIoASooiICKCEKCIiAighioiIAEqIIiIigBKi\niIgIoIQoIiICKCGKiIgASogiIiKAEqKIiAighCgiIgIoIYqIiABKiCIiIoASooiICKCEKCIiAigh\nioiIAEqIIiIigBKiiIgIoIQoIiICKCGKiIgASogiIiKAEqKIiAighCgiIgIoIYqIiABKiCIiIoAS\nooiICKCEKCIiAighioiIAEqIIiIigBKiiIgIoIQoIiICKCGKiIgASogiIiJAERKimf3CzNab2att\nrJ9sZpvN7OXUdF2hY8yXurq6YoeQM8WaP6UUbynFCqUVr2KNnmJcId4BfCbLNk+7+8TUNKcQQRVC\nKZ1UijV/SineUooVSitexRo9BU+I7v4n4L0sm1khYhEREWkS1XuIR5vZIjN72MwOK3YwIiLS/Zm7\nF/6gZpXAQnf/aCvryoGku39gZlOAm9390Db2U/jgRUQk8ty9wz2NvfIRSFe4+7a0+Vozm29mg919\nUyvbqmtVRERCUawuU6ON+4RmNjxt/iiCq9gWyVBERCRMBb9CNLPfAlXAEDOrB2YBZYC7+wLgDDO7\nGNgFbAfOLHSMIiLS8xTlHqKIiEjURHWUaQtmFks9qP9gG+tvMbM3U6NTP1bo+DJiaTPWqBUeMLNV\nZrbYzF4xsxfb2CYSbZst1gi27QAzqzazpWb2mpl9opVtotK27cYalbY1s0NTf/8vp/5838wua2W7\nqLRr1nij0rapWK40s7+Z2atmdreZlbWyTVTatt1YO9Wu7l4SE3Al8BvgwVbWTQEeTs1/Ang+wrFO\nbm15EWNdAQxqZ31k2jaHWKPWtr8CZqTmewH7Rbhts8UaqbZNxRQD1gGjo9quOcYbibYFRqT+jZWl\nPt8LnBvFts0x1g63a0lcIZrZKOBE4PY2NjkFuAvA3V8ABqQPzimkHGKFaBUeMNrvKYhM25I91qZt\nis7M9gOOd/c7ANx9t7tvydgsEm2bY6wQkbZN86/Acndfk7E8Eu3airbihei0bRzoZ2a9gL4ECTxd\nlNo2W6zQwXYtiYQIzAO+CbR1w3MkkH6SrU0tK4ZssUK0Cg848LiZvWRmF7SyPkptmy1WiE7bjgU2\nmNkdqe6aBWbWJ2ObqLRtLrFCdNq2yZnAPa0sj0q7ZmorXohA27r7OuBHQD1Bm2129z9kbBaJts0x\nVuhgu0Y+IZrZ54D17r6Idh7XiIIcY/0LUOHuHwN+AtQUMMTWHOvuEwmuai81s+OKHE97ssUapbbt\nBUwEfpqK+QPg20WMpz25xBqltsXMegMnA9XFjCNXWeKNRNua2UCCK8BKgi7JcjM7uxixZJNjrB1u\n18gnROBY4GQzW0Hwv6tPmtldGdusBUanfR6VWlZoWWN1923u/kFqvhbobWaDCx9qczxvpf58F3gA\nOCpjk6i0bdZYI9a2/wDWuPufU5/vJ0g66aLStlljjVjbQnAv6y+pcyFTVNo1XZvxRqht/xVY4e6b\n3D0B/A44JmObqLRt1lg7066RT4jufq27V7j7QcB04El3PzdjsweBcwHMbBLB5fP6AoeaU6wWocID\nZtbXglJ5mFk/4ATgbxmbRaJtc4k1Sm2baqM1ZtZUdvDTwJKMzSLRtrnEGqW2TTmLtrsfI9GuGdqM\nN0JtWw9MMrN9zcwIzoOlGdtEpW2zxtqZdo1c6bZcmdmFpB7md/dHzOxEM/s70ADMKHJ4e0mPlWgV\nHhgOPGBBTdhewN3u/lhE2zZrrESrbQEuA+5OdZetAGZEtG2zxkqE2tbM+hJcIXwtbVlU2zVrvESk\nbd39RTO7H3glFcvLwIIotm0usdKJdtWD+SIiIpRAl6mIiEghKCGKiIighCgiIgIoIYqIiABKiCIi\nIoASooiICKCEKNLMzM4zsz+b2RYz25Sq6/mjIseUNLNLsmyz0szmFiqmbMxsmJnNMrOKjOWTU98n\nCnVQRVpQQhQBzOw7wM+BWuA04EsEtQ9PKmZcJWp/YBYwppV1evBZIqtkK9WIhOxS4Gfufn3asoeB\n7xUpnlJmKPFJCdIVokhgIJC1JqOZ7WNmc82s3sx2pF4tMyVjm5Vm9kMzu87M3jKzrWb2GwveO9i0\nTV8z+y8ze93MGsxshZn9xMz65+G7YWbHm1ld6lgbLHjFU3na+vNS3ZmHm9ljZrbNzJaa2Wmt7Ov7\nZrbegre/325m01M/W2FmlcCrqU3rUssTGbsYZmb3pdpleaq8lkjRKSGKBF4GLjOzc639ivj/Q1Dc\neA4wFXgJeNDMPpqx3VkEBYfPB64EPkfQJdukL0EPzXXAZ1N/fhK4r+tfZW9mdizwOMELVE8HLid4\nhdYv0zZruqK7G/hf4FTgTeAeMxuRtq8rge8A81P72g7cmPbzbwHnEFwlXgxMAo5ODwdYACxKHeMp\n4Cdm9k/hfFuRLnB3TZp6/AQcAfwdSKSmvwGzgf5p23w6te64jJ/9I3Bv2ueVwAagT9qys4HdwIfa\nOH6c4PU1CWBU2vIkcEmW2FcCc9tZ/wzwh4xln0zt+7DU5y+nPn85bZvBBIWRv5b6HCNIqrdk7Ovh\nVNwVqc8fSe3rXzK2m5xaPittWS/gHeAHxT4HNGnSFaII4O5/BcYTvMT1p6nF1wMvpd5WAEFCfBv4\nPzOLp6ZewJNA5hXO4+6+Pe3zAwQJ5Z+bFpjZl1IjWbcSJJ4/pVYdSkgsePP9JKA6LeY48GzqmEem\nbe4EV5LBh+BVOe8QvPMOgvfgHQAszDjMgx0IKfMYuwmuREe1+RMiBaKEKJLi7rvc/WF3v8zdDyfo\n7jwE+Gpqk6HAgQSJpGlqJBhRmfkL/Z2MfW8HtqV+ntS9uTsJEtMZwCcIRrcasG+IX2sQwdXn/Iy4\nd+Cu7dAAAAJbSURBVBBcnY3O2H5zxufGtHgOIEhomS+5be0lve1p7xgiRaNRpiJtcPdfpp7v+3Bq\n0SaCt8ufQpC42rN/+ofUlVo5QZcjBEnweXf/Rto2/xJG3Bk2EySxWcAjraxf18qytrxN8L2HZSzP\n/CxSkpQQRQgeJnf3dzOXAQMIEgHAE8BVQIO7L8uyy38zs77u/kHq8+cJ7p/9OfW5D7Az42e+SMiP\nK7j7B2b2PMG9yzld3N0agrY4hbRuz9TndI2pP3XVJyVFCVEk8Fcz+1/gMYLuzjHA1QRvBb8LwN0f\nN7PHgD+Y2Y3Aa8B+wMeAfdz9u2n72w48bGY3ASOAucDv3P2N1PrHCUZXXgu8QDDq81NdiP9DZnZ6\nxrIGd/89cE0qZgfuB7YClaljXuvuf8/lAO6eNLMfAj80sw0E3b0nA4enNkmm/qwn+P5fNrMtwC53\n/0tqXbYra5GiUUIUCcwmuNK5mWB05dsEv/C/4O6r07Y7DbiW4NGFCoJu1EXAf2Xs778JEs8vgH4E\njzKkl2C7DRgLXEZwJfUYwaMaz2fsx8ntqnFqakq3GjjI3Z9NdcfOJkju8dS635P92cu9ju/u88xs\nEMEjFVemvtcPCAYibUlts9PMzifopv0jwe+ZeNr+2jqOSFGZu85DkTCZ2Uqg2t2vKXYshWBmtwOf\ndvexxY5FpCt0hSgiOTOzjwBnAs8RXNVNIXiGsUckf+nelBBFwpdrN2cpagCOI6j92o+g6/Uad59X\n1KhEQqAuUxEREfRgvoiICKCEKCIiAighioiIAEqIIiIigBKiiIgIAP8/CbsU89EgcOIAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1122acdd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for target, color in zip(iris_df.target.unique(), ['r', 'g', 'b']):\n",
    "    sub_df = iris_df.query('target == @target')\n",
    "    ax.scatter(sub_df.sepal_length.values, sub_df.sepal_width.values, color=color, \n",
    "               label=iris_data.target_names[target], s=30)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlabel('Sepal Length', size=15)\n",
    "ax.set_ylabel('Sepal Width', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a model might have a good chance at being able to distinguish between classes based solely on sepal length and width, and petal length and width. Frequently we'd want to more EDA, but since that isn't the subject of today's lecture we're going to move on to the one that is, modeling. To try and predict the class of an unknown flower with known measurement we are going to use logistic regression. This is a standard linear based model used for classification problems. We are going to import the [logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model from `sklearn` then feed our features and associated labels to an instance of it via the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "y = iris_df.label.values\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all we had to do to set up our model is create an instance of the `LogisticRegression` class and then call the `fit()` method on the passing it the features and their associated targets. Here we stored the features and targets in the variables `X` and `y`, respectively, this is common notation.\n",
    "\n",
    "Now we can see what our model would predict if it just saw data with an unknown label. What would we predict a flower with `sepal length = 4.5`, `sepal_width = 3.3`, `petal_length = 1.6` and `petal_width = 0.2`? We can use the predict method on our newly fitted model to find out! It thinks it's a \"Setosa\". The model will also accept a 2-dimensional array and hand back preditions for each row in a `numpy array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model guess for single point: setosa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['virginica', 'versicolor', 'setosa'], dtype=object)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'Model guess for single point: {}'.format(logistic_model.predict(np.array([[4.5, 3.3, 1.6, 0.2]]))[0])\n",
    "logistic_model.predict(np.array([[7.2, 2.8, 6.6, 2], [6.2, 2.5, 3.6, 2], [4.7, 3.6, 1.9, .1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, now we can see what our model would predict for all of the values that we have! In addition we can see how good it is at predicting with the `score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'virginica', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'virginica', 'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor', 'virginica',\n",
       "       'virginica', 'virginica', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'versicolor', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `score()` method we pass it all the data points we have and their associated labels. The model then predicts what it thinks the class each data point belongs to and compares it to the known label. The number that we got back is the percent that the model guessed correctly. We call this metric the **accuracy**.\n",
    "\n",
    "This is an incredibly high score! But we did use all of the data that we scored on to train the model. For this reason, we refer to this as the **in-sample** accuracy. Frequently we want to see how our model would perform on new data. One way that we can do that is by only using a portion of the data for training and using the remaining for testing. Let's see what that would look like. To make our job easier we're going to import another function from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97368421052631582"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "logistic_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using `train_test_split()` to divide our `X` features and `y` labels into two groups, while maintain their matched information. Be default `train_test_split()` makes the train set ~75% of the data points and the test set the remaining ~25%. You can control it if you'd like, however, check out the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) to see how.\n",
    "\n",
    "This time around we can see that our accuracy score is worse :'( But the model is being tested on data points that are completely unknown to it unlike before. For this reason we refer to this metric as the **out-of-sample** accuracy. And 92% is very good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problems\n",
    "\n",
    "As discussed in the introduction, regression problems enter us into a realm where we want to predict some continuous value based on some inputs. To accomplish this we are going to want some data with a taget that is continuous. We are going to be working with the boston data set today. Just as with the iris data set it is located in the `sklearn.datasets` module. Let's take a look the data set's description and, as before load it into a `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_data = load_boston()\n",
    "print boston_data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "      <td>21.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.54</td>\n",
       "      <td>19.88</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.33</td>\n",
       "      <td>16.30</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>303.42</td>\n",
       "      <td>16.51</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.88</td>\n",
       "      <td>14.81</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>306.38</td>\n",
       "      <td>17.28</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>387.94</td>\n",
       "      <td>12.80</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.23</td>\n",
       "      <td>11.98</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>379.70</td>\n",
       "      <td>18.03</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "      <td>21.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.07</td>\n",
       "      <td>7.74</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>370.73</td>\n",
       "      <td>13.34</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24</td>\n",
       "      <td>666</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.22</td>\n",
       "      <td>11.45</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4</td>\n",
       "      <td>711</td>\n",
       "      <td>20.1</td>\n",
       "      <td>395.09</td>\n",
       "      <td>18.06</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4</td>\n",
       "      <td>711</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4</td>\n",
       "      <td>711</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4</td>\n",
       "      <td>711</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4</td>\n",
       "      <td>711</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "      <td>20.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>393.29</td>\n",
       "      <td>17.60</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "      <td>18.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6</td>\n",
       "      <td>391</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         crim    zn  indus  chas    nox     rm    age     dis  rad  tax  \\\n",
       "0     0.00632  18.0   2.31     0  0.538  6.575   65.2  4.0900    1  296   \n",
       "1     0.02731   0.0   7.07     0  0.469  6.421   78.9  4.9671    2  242   \n",
       "2     0.02729   0.0   7.07     0  0.469  7.185   61.1  4.9671    2  242   \n",
       "3     0.03237   0.0   2.18     0  0.458  6.998   45.8  6.0622    3  222   \n",
       "4     0.06905   0.0   2.18     0  0.458  7.147   54.2  6.0622    3  222   \n",
       "5     0.02985   0.0   2.18     0  0.458  6.430   58.7  6.0622    3  222   \n",
       "6     0.08829  12.5   7.87     0  0.524  6.012   66.6  5.5605    5  311   \n",
       "7     0.14455  12.5   7.87     0  0.524  6.172   96.1  5.9505    5  311   \n",
       "8     0.21124  12.5   7.87     0  0.524  5.631  100.0  6.0821    5  311   \n",
       "9     0.17004  12.5   7.87     0  0.524  6.004   85.9  6.5921    5  311   \n",
       "10    0.22489  12.5   7.87     0  0.524  6.377   94.3  6.3467    5  311   \n",
       "11    0.11747  12.5   7.87     0  0.524  6.009   82.9  6.2267    5  311   \n",
       "12    0.09378  12.5   7.87     0  0.524  5.889   39.0  5.4509    5  311   \n",
       "13    0.62976   0.0   8.14     0  0.538  5.949   61.8  4.7075    4  307   \n",
       "14    0.63796   0.0   8.14     0  0.538  6.096   84.5  4.4619    4  307   \n",
       "15    0.62739   0.0   8.14     0  0.538  5.834   56.5  4.4986    4  307   \n",
       "16    1.05393   0.0   8.14     0  0.538  5.935   29.3  4.4986    4  307   \n",
       "17    0.78420   0.0   8.14     0  0.538  5.990   81.7  4.2579    4  307   \n",
       "18    0.80271   0.0   8.14     0  0.538  5.456   36.6  3.7965    4  307   \n",
       "19    0.72580   0.0   8.14     0  0.538  5.727   69.5  3.7965    4  307   \n",
       "20    1.25179   0.0   8.14     0  0.538  5.570   98.1  3.7979    4  307   \n",
       "21    0.85204   0.0   8.14     0  0.538  5.965   89.2  4.0123    4  307   \n",
       "22    1.23247   0.0   8.14     0  0.538  6.142   91.7  3.9769    4  307   \n",
       "23    0.98843   0.0   8.14     0  0.538  5.813  100.0  4.0952    4  307   \n",
       "24    0.75026   0.0   8.14     0  0.538  5.924   94.1  4.3996    4  307   \n",
       "25    0.84054   0.0   8.14     0  0.538  5.599   85.7  4.4546    4  307   \n",
       "26    0.67191   0.0   8.14     0  0.538  5.813   90.3  4.6820    4  307   \n",
       "27    0.95577   0.0   8.14     0  0.538  6.047   88.8  4.4534    4  307   \n",
       "28    0.77299   0.0   8.14     0  0.538  6.495   94.4  4.4547    4  307   \n",
       "29    1.00245   0.0   8.14     0  0.538  6.674   87.3  4.2390    4  307   \n",
       "..        ...   ...    ...   ...    ...    ...    ...     ...  ...  ...   \n",
       "476   4.87141   0.0  18.10     0  0.614  6.484   93.6  2.3053   24  666   \n",
       "477  15.02340   0.0  18.10     0  0.614  5.304   97.3  2.1007   24  666   \n",
       "478  10.23300   0.0  18.10     0  0.614  6.185   96.7  2.1705   24  666   \n",
       "479  14.33370   0.0  18.10     0  0.614  6.229   88.0  1.9512   24  666   \n",
       "480   5.82401   0.0  18.10     0  0.532  6.242   64.7  3.4242   24  666   \n",
       "481   5.70818   0.0  18.10     0  0.532  6.750   74.9  3.3317   24  666   \n",
       "482   5.73116   0.0  18.10     0  0.532  7.061   77.0  3.4106   24  666   \n",
       "483   2.81838   0.0  18.10     0  0.532  5.762   40.3  4.0983   24  666   \n",
       "484   2.37857   0.0  18.10     0  0.583  5.871   41.9  3.7240   24  666   \n",
       "485   3.67367   0.0  18.10     0  0.583  6.312   51.9  3.9917   24  666   \n",
       "486   5.69175   0.0  18.10     0  0.583  6.114   79.8  3.5459   24  666   \n",
       "487   4.83567   0.0  18.10     0  0.583  5.905   53.2  3.1523   24  666   \n",
       "488   0.15086   0.0  27.74     0  0.609  5.454   92.7  1.8209    4  711   \n",
       "489   0.18337   0.0  27.74     0  0.609  5.414   98.3  1.7554    4  711   \n",
       "490   0.20746   0.0  27.74     0  0.609  5.093   98.0  1.8226    4  711   \n",
       "491   0.10574   0.0  27.74     0  0.609  5.983   98.8  1.8681    4  711   \n",
       "492   0.11132   0.0  27.74     0  0.609  5.983   83.5  2.1099    4  711   \n",
       "493   0.17331   0.0   9.69     0  0.585  5.707   54.0  2.3817    6  391   \n",
       "494   0.27957   0.0   9.69     0  0.585  5.926   42.6  2.3817    6  391   \n",
       "495   0.17899   0.0   9.69     0  0.585  5.670   28.8  2.7986    6  391   \n",
       "496   0.28960   0.0   9.69     0  0.585  5.390   72.9  2.7986    6  391   \n",
       "497   0.26838   0.0   9.69     0  0.585  5.794   70.6  2.8927    6  391   \n",
       "498   0.23912   0.0   9.69     0  0.585  6.019   65.3  2.4091    6  391   \n",
       "499   0.17783   0.0   9.69     0  0.585  5.569   73.5  2.3999    6  391   \n",
       "500   0.22438   0.0   9.69     0  0.585  6.027   79.7  2.4982    6  391   \n",
       "501   0.06263   0.0  11.93     0  0.573  6.593   69.1  2.4786    1  273   \n",
       "502   0.04527   0.0  11.93     0  0.573  6.120   76.7  2.2875    1  273   \n",
       "503   0.06076   0.0  11.93     0  0.573  6.976   91.0  2.1675    1  273   \n",
       "504   0.10959   0.0  11.93     0  0.573  6.794   89.3  2.3889    1  273   \n",
       "505   0.04741   0.0  11.93     0  0.573  6.030   80.8  2.5050    1  273   \n",
       "\n",
       "     ptratio       b  lstat  medv  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "5       18.7  394.12   5.21  28.7  \n",
       "6       15.2  395.60  12.43  22.9  \n",
       "7       15.2  396.90  19.15  27.1  \n",
       "8       15.2  386.63  29.93  16.5  \n",
       "9       15.2  386.71  17.10  18.9  \n",
       "10      15.2  392.52  20.45  15.0  \n",
       "11      15.2  396.90  13.27  18.9  \n",
       "12      15.2  390.50  15.71  21.7  \n",
       "13      21.0  396.90   8.26  20.4  \n",
       "14      21.0  380.02  10.26  18.2  \n",
       "15      21.0  395.62   8.47  19.9  \n",
       "16      21.0  386.85   6.58  23.1  \n",
       "17      21.0  386.75  14.67  17.5  \n",
       "18      21.0  288.99  11.69  20.2  \n",
       "19      21.0  390.95  11.28  18.2  \n",
       "20      21.0  376.57  21.02  13.6  \n",
       "21      21.0  392.53  13.83  19.6  \n",
       "22      21.0  396.90  18.72  15.2  \n",
       "23      21.0  394.54  19.88  14.5  \n",
       "24      21.0  394.33  16.30  15.6  \n",
       "25      21.0  303.42  16.51  13.9  \n",
       "26      21.0  376.88  14.81  16.6  \n",
       "27      21.0  306.38  17.28  14.8  \n",
       "28      21.0  387.94  12.80  18.4  \n",
       "29      21.0  380.23  11.98  21.0  \n",
       "..       ...     ...    ...   ...  \n",
       "476     20.2  396.21  18.68  16.7  \n",
       "477     20.2  349.48  24.91  12.0  \n",
       "478     20.2  379.70  18.03  14.6  \n",
       "479     20.2  383.32  13.11  21.4  \n",
       "480     20.2  396.90  10.74  23.0  \n",
       "481     20.2  393.07   7.74  23.7  \n",
       "482     20.2  395.28   7.01  25.0  \n",
       "483     20.2  392.92  10.42  21.8  \n",
       "484     20.2  370.73  13.34  20.6  \n",
       "485     20.2  388.62  10.58  21.2  \n",
       "486     20.2  392.68  14.98  19.1  \n",
       "487     20.2  388.22  11.45  20.6  \n",
       "488     20.1  395.09  18.06  15.2  \n",
       "489     20.1  344.05  23.97   7.0  \n",
       "490     20.1  318.43  29.68   8.1  \n",
       "491     20.1  390.11  18.07  13.6  \n",
       "492     20.1  396.90  13.35  20.1  \n",
       "493     19.2  396.90  12.01  21.8  \n",
       "494     19.2  396.90  13.59  24.5  \n",
       "495     19.2  393.29  17.60  23.1  \n",
       "496     19.2  396.90  21.14  19.7  \n",
       "497     19.2  396.90  14.10  18.3  \n",
       "498     19.2  396.90  12.92  21.2  \n",
       "499     19.2  395.77  15.10  17.5  \n",
       "500     19.2  396.90  14.33  16.8  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [name.lower() for name in boston_data.feature_names]\n",
    "boston_df = pd.DataFrame(boston_data.data, columns=column_names)\n",
    "boston_df['medv'] = boston_data.target\n",
    "boston_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
